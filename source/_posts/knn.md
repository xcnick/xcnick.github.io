title: k近邻算法
date: 2018-11-29 14:12:45
tags: MachineLearning
mathjax: true
---

> k近邻算法
<!-- more -->
# 概述

k近邻算法是一种基本分类和回归方法。给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

k近邻算法没有显式的学习过程，实际上是利用训练集对特征向量进行划分，并作为其分类的模型。`k值选择`、`距离度量`、`分类决策规则`是k近邻算法的三个基本要素。

## k值选择

* 选取较小的k值，意味着整体模型会变得复杂，容易发生过拟合。近似误差减小，估计误差变大。
* 选取较大的k值，模型变得简单，容易出现预测错误。近似误差增大，估计误差减小。
* k值一般取一个比较小的数值，使用交叉验证法选取最优的k值。


## 距离的度量

* 距离的定义
$$L_p(x_i, x_j) = (\sum_{i=1}^{n}|x_i^l - x_j^l|^p)^\frac{1}{p}$$

当$p=2$时，称为欧氏距离。

当$p=1$时，称为曼哈顿距离。

## 特征归一化

归一化的分母为每个特征的最大值-最小值。
$$ sign(x) =
\begin{cases}
+1, & x \ge 0 \\
-1, & x \lt 0
\end{cases}
$$
* 损失函数
$$minL(w, b) = - \sum_{x_i}{y_i}(w * x + b)$$
* 使用`随机梯度下降`法进行对$w$,$b$进行更新
$$w = w + \eta{y_i}{x_i}$$
$$b = b + \eta{y_i}$$