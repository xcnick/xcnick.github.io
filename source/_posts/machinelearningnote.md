title: 《机器学习》笔记
date: 2017-01-16 20:26:43
tags: MachineLearning
---

> 《机器学习》笔记
<!-- more -->
# 模型评估与选择

## 本质
* 追求泛化误差低。
* 过拟合无法避免。

## 评估方法
* 留出法：直接将数据集划分为两个互斥的集合，其中一个为训练集，另一个为测试集。注意需要保持数据分布的一致性，可使用分层采样法。
* 交叉验证法：将数据集分为k个大小相似的互斥子集，每次用k-1个子集的并集作为训练集，剩下的作为测试集。这样可以获得k组训练/测试集。特殊情况：留一法。
* 自助法：对数据集重复采样，约有36.8%的数据不会被采集，则可留作测试集。

## 性能度量
回归任务最常用的性能度量是`均方误差`；
* 错误率、精度
* 查准率、查全率：一对矛盾的度量
* F1度量，更一般的是FB度量
* ROC与AUC：ROC（受试者工作特征），AUC是ROC曲线下的面积
* 代价敏感错误率与代价曲线：为了权衡不同类型错误造成的不同损失，称为“非均等代价”

## 比较检验

### 偏差与方差
泛化误差可分解为偏差(bias)、方差(variance)与噪声之和
* 偏差度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力
* 方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响
* 噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下限，即刻画了学习问题本身的难度
* 方差与偏差是有冲突的。训练不足时，偏差主导泛化误差；随着训练深入，方差主导泛化误差。

---

# 线性模型

## 基本形式
$$f(x) = w^Tx + b$$

## 线性回归
* 假设输入属性的数目只有1个，则线性回归试图学得：
$$f(x_i) = wx_i + b$$
使得$f(x_i) = y_i$

---

# 感知机模型

感知机是二类分类的线性分类模型，输入为样本的特征向量，输出为样本类别，取+1和-1二值，属于判别模型。

* 输入空间到输出空间函数：
$$f(x) = sign(w*x + b)$$
其中$w$为权值，$b$为偏置，$w * x$表示向量内积，$sign$是指符号函数：
$$ sign(x) =
\begin{cases}
+1, & x \ge 0 \\
-1, & x \lt 0
\end{cases}
$$
* 损失函数
$$minL(w, b) = - \sum_{x_i}{y_i}(w * x + b)$$
* 使用`随机梯度下降`法进行对$w$,$b$进行更新
$$w = w + \eta{y_i}{x_i}$$
$$b = b + \eta{y_i}$$

---

# k近邻算法

k近邻算法是一种基本分类和回归方法。给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。

k近邻算法没有显式的学习过程，实际上是利用训练集对特征向量进行划分，并作为其分类的模型。`k值选择`、`距离度量`、`分类决策规则`是k近邻算法的三个基本要素。

## k值选择

* 选取较小的k值，意味着整体模型会变得复杂，容易发生过拟合。近似误差减小，估计误差变大。
* 选取较大的k值，模型变得简单，容易出现预测错误。近似误差增大，估计误差减小。
* k值一般取一个比较小的数值，使用交叉验证法选取最优的k值。

## 距离的度量

* 距离的定义
$$L_p(x_i, x_j) = (\sum_{i=1}^{n}|x_i^l - x_j^l|^p)^\frac{1}{p}$$

当$p=2$时，称为欧氏距离。

当$p=1$时，称为曼哈顿距离。

## 特征归一化

归一化的分母为每个特征的最大值-最小值。
$$ sign(x) =
\begin{cases}
+1, & x \ge 0 \\
-1, & x \lt 0
\end{cases}
$$
* 损失函数
$$minL(w, b) = - \sum_{x_i}{y_i}(w * x + b)$$
* 使用`随机梯度下降`法进行对$w$,$b$进行更新
$$w = w + \eta{y_i}{x_i}$$
$$b = b + \eta{y_i}$$